import os
import io
import json
import torch
import sys
import logging
import inspect
import subprocess
import contextlib
# Import ƒë·ªông ƒë·ªÉ tr√°nh l·ªói khi ch∆∞a c√†i ƒë·∫∑t
from datasets import load_dataset, Dataset
from IPython.display import display
from huggingface_hub import login, HfApi
from pathlib import Path
from jinja2 import Template

# FastLanguageModel s·∫Ω ƒë∆∞·ª£c import ƒë·ªông trong c√°c h√†m c·∫ßn d√πng

MODEL_NAME = None
AUTHOR = None
HF_TOKEN = None
_CURRENT_TOOL = None  # Track tool ƒëang ƒë∆∞·ª£c d√πng: "unsloth" ho·∫∑c "llamafactory"


def set(name, author):
    global MODEL_NAME, AUTHOR
    MODEL_NAME = name
    AUTHOR = author


def hf(token):
    global HF_TOKEN
    HF_TOKEN = token
    login(HF_TOKEN, add_to_git_credential=True)


def identify_dataset(record):
    global MODEL_NAME, AUTHOR
    record["output"] = (
        record["output"]
        .replace("Gemma-tvts", MODEL_NAME)
        .replace("Long Nguyen", AUTHOR)
    )
    return record


# ============================================
# H√ÄM QU·∫¢N L√ù PACKAGES ƒê·ªòNG
# ============================================

def setup_unsloth(force_reinstall=False):
    """
    Setup packages cho Unsloth - uninstall LLaMA-Factory v√† install packages cho Unsloth
    
    Args:
        force_reinstall: N·∫øu True, s·∫Ω reinstall l·∫°i ngay c·∫£ khi ƒë√£ setup
    """
    global _CURRENT_TOOL
    
    if _CURRENT_TOOL == "unsloth" and not force_reinstall:
        print("‚úÖ Unsloth ƒë√£ ƒë∆∞·ª£c setup s·∫µn, b·ªè qua...")
        return
    
    print("=" * 80)
    print("üîß ƒêANG SETUP PACKAGES CHO UNSLOTH")
    print("=" * 80)
    
    # Uninstall LLaMA-Factory n·∫øu c√≥
    print("\nüì¶ ƒêang uninstall LLaMA-Factory...")
    try:
        subprocess.run(["pip", "uninstall", "-y", "llamafactory-cli", "llamafactory"], 
                      check=False, capture_output=True)
        print("‚úÖ ƒê√£ uninstall LLaMA-Factory")
    except:
        pass
    
    # Install packages cho Unsloth
    print("\nüì¶ ƒêang install packages cho Unsloth...")
    unsloth_packages = [
        "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git",
        "trl",
        "transformers",
        "peft",
        "accelerate",
        "bitsandbytes",
        "datasets",
    ]
    
    for package in unsloth_packages:
        try:
            subprocess.run(["pip", "install", "-q", "--upgrade", package], check=False)
        except:
            pass
    
    print("‚úÖ ƒê√£ setup packages cho Unsloth xong!")
    print("=" * 80)
    print()
    
    _CURRENT_TOOL = "unsloth"


def setup_llamafactory(force_reinstall=False):
    """
    Setup packages cho LLaMA-Factory - uninstall Unsloth v√† install packages cho LLaMA-Factory
    
    Args:
        force_reinstall: N·∫øu True, s·∫Ω reinstall l·∫°i ngay c·∫£ khi ƒë√£ setup
    """
    global _CURRENT_TOOL
    
    if _CURRENT_TOOL == "llamafactory" and not force_reinstall:
        print("‚úÖ LLaMA-Factory ƒë√£ ƒë∆∞·ª£c setup s·∫µn, b·ªè qua...")
        return
    
    print("=" * 80)
    print("üîß ƒêANG SETUP PACKAGES CHO LLaMA-FACTORY")
    print("=" * 80)
    
    # Uninstall Unsloth n·∫øu c√≥
    print("\nüì¶ ƒêang uninstall Unsloth...")
    try:
        subprocess.run(["pip", "uninstall", "-y", "unsloth"], 
                      check=False, capture_output=True)
        print("‚úÖ ƒê√£ uninstall Unsloth")
    except:
        pass
    
    # Uninstall trl n·∫øu c√≥ (c√≥ th·ªÉ conflict v·ªõi LLaMA-Factory)
    try:
        subprocess.run(["pip", "uninstall", "-y", "trl"], 
                      check=False, capture_output=True)
    except:
        pass
    
    # Clone LLaMA-Factory n·∫øu ch∆∞a c√≥
    if not os.path.exists("/content/LLaMA-Factory"):
        print("\nüì• ƒêang clone LLaMA-Factory...")
        subprocess.run(["git", "clone", "https://github.com/hiyouga/LLaMA-Factory.git", "/content/LLaMA-Factory"], 
                      check=False)
        print("‚úÖ ƒê√£ clone LLaMA-Factory")
    
    # Install LLaMA-Factory
    print("\nüì¶ ƒêang install LLaMA-Factory...")
    os.chdir("/content/LLaMA-Factory")
    subprocess.run(["pip", "install", "-e", ".[torch,bitsandbytes]"], check=False)
    os.chdir("/content")
    
    print("‚úÖ ƒê√£ setup packages cho LLaMA-Factory xong!")
    print("=" * 80)
    print()
    
    _CURRENT_TOOL = "llamafactory"


def preprocess_dataset(dataset, num_to_train=None):
    """Preprocess dataset cho LLaMA-Factory - t·ª± ƒë·ªông setup packages"""
    setup_llamafactory()
    
    dataset_df = dataset.to_pandas()
    if num_to_train is not None:
        dataset_df = dataset_df.head(num_to_train)
    dataset_df["input"] = dataset_df["input"].fillna("")
    caller_locals = inspect.stack()[1][0].f_locals
    dataset_name = [name for name, val in caller_locals.items() if val is dataset][0]
    
    # T·∫°o th∆∞ m·ª•c data n·∫øu ch∆∞a c√≥
    os.makedirs("/content/LLaMA-Factory/data", exist_ok=True)
    
    file_path = f"/content/LLaMA-Factory/data/{dataset_name}.json"
    dataset_df.to_json(file_path, orient="records", force_ascii=False, indent=4)
    return file_path


def dataset_info(*datasets):
    """T·∫°o file dataset_info.json cho LLaMA-Factory - t·ª± ƒë·ªông setup packages"""
    setup_llamafactory()
    
    info = {}
    for dataset in datasets:
        caller_locals = inspect.stack()[1][0].f_locals
        dataset_name = [name for name, val in caller_locals.items() if val is dataset][
            0
        ]
        info[dataset_name] = {"file_name": f"{dataset_name}.json"}
    
    # T·∫°o th∆∞ m·ª•c data n·∫øu ch∆∞a c√≥
    os.makedirs("/content/LLaMA-Factory/data", exist_ok=True)
    
    file_path = "/content/LLaMA-Factory/data/dataset_info.json"
    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(info, f, ensure_ascii=False, indent=2)
    return file_path


def train(datasets, num_train_epochs, continue_training=True):
    """Train model v·ªõi LLaMA-Factory - t·ª± ƒë·ªông setup packages"""
    setup_llamafactory()
    
    caller_locals = inspect.stack()[1][0].f_locals
    dataset_names = ",".join(
        [
            name
            for dataset in datasets
            for name, val in caller_locals.items()
            if val is dataset
        ]
    )

    if not continue_training:
        os.system("rm -rf /content/LLaMA-Factory/gemma_lora")

    args = dict(
        stage="sft",
        do_train=True,
        model_name_or_path="ura-hcmut/GemSUra-2B",
        dataset=dataset_names,
        template="gemma",
        finetuning_type="lora",
        lora_target="all",
        output_dir="gemma_lora",
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        lr_scheduler_type="cosine",
        logging_steps=10,
        warmup_ratio=0.1,
        save_steps=1000,
        learning_rate=5e-5,
        num_train_epochs=num_train_epochs,
        max_samples=500,
        max_grad_norm=1.0,
        quantization_bit=4,
        loraplus_lr_ratio=16.0,
        fp16=True,
    )

    file_path = "/content/LLaMA-Factory/train_gemma.json"

    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(args, f, ensure_ascii=False, indent=4)

    os.chdir("/content/LLaMA-Factory")
    
    # ƒê·∫£m b·∫£o LLaMA-Factory ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t (ƒë√£ setup trong setup_llamafactory())
    # Ch·ªâ c√†i l·∫°i n·∫øu c·∫ßn thi·∫øt
    process = subprocess.Popen(
        ["llamafactory-cli", "train", "train_gemma.json"],
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
    )

    start_printing_all = False

    for line in iter(process.stdout.readline, b""):
        decoded_line = line.decode()
        if "train metrics" in decoded_line.lower():
            start_printing_all = True
        if "loss" in decoded_line.lower() or start_printing_all:
            print(decoded_line, end="")

    process.stdout.close()
    process.wait()


class SuppressLogging:
    def __enter__(self):
        logging.disable(logging.CRITICAL)

    def __exit__(self, exc_type, exc_val, exc_tb):
        logging.disable(logging.NOTSET)


def test():
    """Test model ƒë√£ ƒë∆∞·ª£c train v·ªõi LLaMA-Factory - t·ª± ƒë·ªông setup packages"""
    setup_llamafactory()
    
    os.chdir("/content/LLaMA-Factory/src")
    from llamafactory.chat import ChatModel
    from llamafactory.extras.misc import torch_gc

    os.chdir("/content/LLaMA-Factory")

    args = dict(
        model_name_or_path="ura-hcmut/GemSUra-2B",
        adapter_name_or_path="gemma_lora",
        template="gemma",
        finetuning_type="lora",
        quantization_bit=4,
    )

    with SuppressLogging():
        chat_model = ChatModel(args)

    print("***** Nh·∫≠p clear ƒë·ªÉ x√≥a l·ªãch s·ª≠ tr√≤ chuy·ªán, nh·∫≠p exit ƒë·ªÉ tho√°t nha! *****")
    messages = []
    while True:
        query = input("\nNg∆∞·ªùi d√πng: ")
        if query.strip().lower() == "exit":
            break
        if query.strip().lower() == "clear":
            messages = []
            torch_gc()
            print("L·ªãch s·ª≠ tr√≤ chuy·ªán v·ª´a ƒë∆∞·ª£c x√≥a.")
            continue

        messages.append({"role": "user", "content": query})
        print(f"Tr·ª£ l√Ω: ", end="", flush=True)

        response = ""
        for new_text in chat_model.stream_chat(messages):
            print(new_text, end="", flush=True)
            response += new_text
        print()
        messages.append({"role": "assistant", "content": response})
    torch_gc()


def merge_and_push(repo_id):
    """Merge v√† push model l√™n Hugging Face v·ªõi LLaMA-Factory - t·ª± ƒë·ªông setup packages"""
    setup_llamafactory()
    
    os.chdir("/content/LLaMA-Factory/")

    args = dict(
        model_name_or_path="ura-hcmut/GemSUra-2B",
        adapter_name_or_path="gemma_lora",
        template="gemma",
        finetuning_type="lora",
        export_dir="gemma_lora_merged",
        export_size=2,
        export_device="cpu",
    )

    with open("gemma_lora_merged.json", "w", encoding="utf-8") as f:
        json.dump(args, f, ensure_ascii=False, indent=2)

    with SuppressLogging(), open(os.devnull, "w") as devnull:
        subprocess.run(
            ["llamafactory-cli", "export", "gemma_lora_merged.json"],
            stdout=devnull,
            stderr=devnull,
            check=True,
        )

    print("***** ƒê√£ merge model th√†nh c√¥ng v√† ti·∫øn h√†nh upload l√™n Huggingface! *****")

    model_dir = "/content/LLaMA-Factory/gemma_lora_merged"
    tokenizer_dir = "/content/LLaMA-Factory/gemma_lora"

    tokenizer_config_path = Path(tokenizer_dir) / "tokenizer_config.json"
    with open(tokenizer_config_path, "r", encoding="utf-8") as f:
        tokenizer_config = json.load(f)
    tokenizer_config.pop("chat_template", None)
    with open(tokenizer_config_path, "w", encoding="utf-8") as f:
        json.dump(tokenizer_config, f, ensure_ascii=False, indent=4)

    tokenizer_files = [
        "tokenizer.json",
        "tokenizer.model",
        "tokenizer_config.json",
        "special_tokens_map.json",
    ]

    api = HfApi()
    global HF_TOKEN

    for file in os.listdir(model_dir):
        file_path = Path(model_dir) / file
        api.upload_file(
            path_or_fileobj=file_path,
            path_in_repo=file,
            repo_id=repo_id,
            repo_type="model",
            token=HF_TOKEN,
        )

    for file_name in tokenizer_files:
        file_path = Path(tokenizer_dir) / file_name
        api.upload_file(
            path_or_fileobj=file_path,
            path_in_repo=file_name,
            repo_id=repo_id,
            repo_type="model",
            token=HF_TOKEN,
        )


model = None
tokenizer = None
messages = []


def inference(model_name, max_seq_length=2048, dtype=None, load_in_4bit=True):
    """Load model v·ªõi unsloth ƒë·ªÉ inference - t·ª± ƒë·ªông setup packages"""
    setup_unsloth()
    
    # Import ƒë·ªông sau khi setup
    from unsloth import FastLanguageModel
    
    logging.getLogger().setLevel(logging.ERROR)
    global model, tokenizer

    try:
        model, tokenizer = FastLanguageModel.from_pretrained(
            model_name=model_name,
            max_seq_length=max_seq_length,
            dtype=dtype,
            load_in_4bit=load_in_4bit,
        )
        FastLanguageModel.for_inference(model)
        print(f"‚úÖ ƒê√£ load model {model_name} th√†nh c√¥ng!")
    except Exception as e:
        print(f"L·ªói khi load model: {e}")
        print("B·∫°n ch·ªâ c·∫ßn ch·∫°y inference m·ªôt l·∫ßn duy nh·∫•t, b·∫°n kh√¥ng c·∫ßn ch·∫°y l·∫°i!")


def chat(max_new_tokens=128, history=True):
    """Chat v·ªõi model ƒë√£ load b·∫±ng unsloth - t·ª± ƒë·ªông setup packages"""
    setup_unsloth()
    
    # Import ƒë·ªông sau khi setup
    from unsloth import FastLanguageModel
    
    global model, tokenizer, messages

    chat_template = """{{ '<bos>' }}{% if messages[0]['role'] == 'system' %}{% set system_message = messages[0]['content'] %}{% endif %}{% if system_message is defined %}{{ system_message }}{% endif %}{% for message in messages %}{% set content = message['content'] %}{% if message['role'] == 'user' %}{{ '<start_of_turn>user\n' + content + '<end_of_turn>\n<start_of_turn>model\n' }}{% elif message['role'] == 'assistant' %}{{ content + '<end_of_turn>\n' }}{% endif %}{% endfor %}"""

    messages = []

    while True:
        query = input("\nNg∆∞·ªùi d√πng: ")
        if query.strip().lower() == "exit":
            break
        if query.strip().lower() == "clear":
            messages = []
            print("L·ªãch s·ª≠ tr√≤ chuy·ªán v·ª´a ƒë∆∞·ª£c x√≥a.")
            continue

        if history:
            messages.append({"role": "user", "content": query})
        else:
            messages = [{"role": "user", "content": query}]

        template = Template(chat_template)
        input_text = template.render(messages=messages)

        print(f"Tr·ª£ l√Ω: ", end="", flush=True)

        inputs = tokenizer(input_text, return_tensors="pt").to("cpu")

        outputs = model.generate(
            **inputs, max_new_tokens=max_new_tokens, use_cache=True
        )

        decoded_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        if "model" in decoded_text:
            response = decoded_text.split("model")[-1].strip()
        else:
            response = decoded_text.strip()
        print(response)

        if history:
            messages.append({"role": "assistant", "content": response})


def quantize_and_push(repo_id):
    """Quantize model sang GGUF format v√† push l√™n Hugging Face - t·ª± ƒë·ªông setup packages"""
    setup_unsloth()
    
    # Import ƒë·ªông sau khi setup
    from unsloth import FastLanguageModel
    
    logging.getLogger("unsloth").setLevel(logging.CRITICAL)
    original_stdout = sys.stdout
    original_stderr = sys.stderr
    temp_stdout = io.StringIO()
    os.chdir("/content")

    global model, tokenizer, HF_TOKEN
    try:
        with contextlib.redirect_stdout(temp_stdout), contextlib.redirect_stderr(
            temp_stdout
        ):
            model.push_to_hub_gguf(
                repo_id, tokenizer, token=HF_TOKEN
            )
    except Exception as e:
        sys.stdout = original_stdout
        sys.stderr = original_stderr
        return
    finally:
        temp_stdout.seek(0)
        output_lines = temp_stdout.readlines()

    sys.stdout = original_stdout
    sys.stderr = original_stderr

    start_printing = False
    for line in output_lines:
        if "main: quantize time" in line.lower():
            start_printing = True
        if start_printing:
            print(line, end="")


def thank_you_and_good_luck():
    art = [
        "‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£∞‚£Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä",
        "‚¢Ä‚£Ä‚†Ä‚†Ä‚†Ä‚¢Ä‚£Ñ‚†ò‚†Ä‚†Ä‚£∂‚°ø‚£∑‚£¶‚£æ‚£ø‚£ß",
        "‚¢∫‚£æ‚£∂‚£¶‚£∞‚°ü‚£ø‚°á‚†Ä‚†Ä‚†ª‚£ß‚†Ä‚†õ‚†Ä‚°ò‚†è",
        "‚†à‚¢ø‚°Ü‚†â‚†õ‚†Å‚°∑‚†Å‚†Ä‚†Ä‚†Ä‚†â‚†≥‚£¶‚£Æ‚†Å‚†Ä",
        "‚†Ä‚†Ä‚†õ‚¢∑‚£Ñ‚£º‚†É‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä‚†â‚†Ä‚††‚°ß",
        "‚†Ä‚†Ä‚†Ä‚†Ä‚†â‚†ã‚†Ä‚†Ä‚†Ä‚††‚°•‚†Ñ‚†Ä‚†Ä‚†Ä‚†Ä‚†Ä",
        "",
        "Ch√∫c c√°c b·∫°n c√≥ m·ªôt tr·∫£i nghi·ªám tuy·ªát v·ªùi v√† ƒë√°ng nh·ªõ t·∫°i Tr·∫°i h√® CSE Summer School 2024 nh√©!",
    ]

    for line in art:
        print(line)